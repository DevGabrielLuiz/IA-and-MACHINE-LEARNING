{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning / fine-tuning\n",
    "This tutorial will guide you through the process of using transfer learning to learn an accurate image classifier from a relatively small number of training samples. Generally speaking, transfer learning refers to the process of leveraging the knowledge learned in one model for the training of another model.\n",
    "\n",
    "More specifically, the process involves taking an existing neural network which was previously trained to good performance on a larger dataset, and using it as the basis for a new model which leverages that previous network's accuracy for a new task. This method has become popular in recent years to improve the performance of a neural net trained on a small dataset; the intuition is that the new dataset may be too small to train to good performance by itself, but we know that most neural nets trained to learn image features often learn similar features anyway, especially at early layers where they are more generic (edge detectors, blobs, and so on).\n",
    "\n",
    "Transfer learning has been largely enabled by the open-sourcing of state-of-the-art models; for the top performing models in image classification tasks (like from ILSVRC), it is common practice now to not only publish the architecture, but to release the trained weights of the model as well. This lets amateurs use these top image classifiers to boost the performance of their own task-specific models.\n",
    "\n",
    "Feature extraction vs. fine-tuning\n",
    "At one extreme, transfer learning can involve taking the pre-trained network and freezing the weights, and using one of its hidden layers (usually the last one) as a feature extractor, using those features as the input to a smaller neural net.\n",
    "\n",
    "At the other extreme, we start with the pre-trained network, but we allow some of the weights (usually the last layer or last few layers) to be modified. Another name for this procedure is called \"fine-tuning\" because we are slightly adjusting the pre-trained net's weights to the new task. We usually train such a network with a lower learning rate, since we expect the features are already relatively good and do not need to be changed too much.\n",
    "\n",
    "Sometimes, we do something in-between: Freeze just the early/generic layers, but fine-tune the later layers. Which strategy is best depends on the size of your dataset, the number of classes, and how much it resembles the dataset the previous model was trained on (and thus, whether it can benefit from the same learned feature extractors). A more detailed discussion of how to strategize can be found in [1] [2].\n",
    "\n",
    "Procedure\n",
    "In this guide will go through the process of loading a state-of-the-art, 1000-class image classifier, VGG16 which won the ImageNet challenge in 2014, and using it as a fixed feature extractor to train a smaller custom classifier on our own images, although with very few code changes, you can try fine-tuning as well.\n",
    "\n",
    "We will first load VGG16 and remove its final layer, the 1000-class softmax classification layer specific to ImageNet, and replace it with a new classification layer for the classes we are training over. We will then freeze all the weights in the network except the new ones connecting to the new classification layer, and then train the new classification layer over our new dataset.\n",
    "\n",
    "We will also compare this method to training a small neural network from scratch on the new dataset, and as we shall see, it will dramatically improve our accuracy. We will do that part first.\n",
    "\n",
    "As our test subject, we'll use a dataset consisting of around 6000 images belonging to 97 classes, and train an image classifier with around 80% accuracy on it. It's worth noting that this strategy scales well to image sets where you may have even just a couple hundred or less images. Its performance will be lesser from a small number of samples (depending on classes) as usual, but still impressive considering the usual constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "# if using Theano with GPU\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Downloading 101_Object_Categories for image notebooks\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "#=#=#                                                                          \n",
      "##O#-#                                                                         \n",
      "##O=#  #                                                                       \n",
      "##############                                                            20.3%\n",
      "######################################################################## 100.0%\n",
      "tar: Error opening archive: Unrecognized archive format\n",
      "'rm' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
      "'ls' n�o � reconhecido como um comando interno\n",
      "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "!echo \"Downloading 101_Object_Categories for image notebooks\"\n",
    "!curl -L -o 101_ObjectCategories.tar.gz --progress-bar https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
    "!tar -xzf 101_ObjectCategories.tar.gz\n",
    "!rm 101_ObjectCategories.tar.gz\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kagglecatsanddogs_5340/PetImages/Cat', 'kagglecatsanddogs_5340/PetImages/Dog']\n"
     ]
    }
   ],
   "source": [
    "root = 'kagglecatsanddogs_5340/PetImages/'\n",
    "\n",
    "train_split, val_split = 0.7, 0.15\n",
    "\n",
    "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
    "categories = [c for c in categories if c not in [os.path.join(root) ]]\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função é útil para pré-processar os dados em uma imagem e um vetor de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load image and return it and input vector\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregue todas as imagens da pasta raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x000001DFFB113790>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dp, f) \u001b[38;5;28;01mfor\u001b[39;00m dp, dn, filenames \n\u001b[0;32m      4\u001b[0m           \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(category) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames \n\u001b[0;32m      5\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(f)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m----> 7\u001b[0m     img, x \u001b[38;5;241m=\u001b[39m \u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39marray(x[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:c})\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mget_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image\u001b[39m(path):\n\u001b[1;32m----> 3\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[0;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus X550L\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:236\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 236\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mpil_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be path-like or io.BytesIO, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus X550L\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3536\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3534\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3535\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3536\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x000001DFFB113790>"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for c, category in enumerate(categories):\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames \n",
    "              in os.walk(category) for f in filenames \n",
    "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "    for img_path in images:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':c})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize a ordem dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criar divisão de treinamento/validação/teste (70%, 15%, 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_val = int(train_split * len(data))\n",
    "idx_test = int((train_split + val_split) * len(data))\n",
    "train = data[:idx_val]\n",
    "val = data[idx_val:idx_test]\n",
    "test = data[idx_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados separados para rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
    "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
    "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
